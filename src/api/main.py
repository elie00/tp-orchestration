"""
API FastAPI pour le service de d√©tection et reconnaissance de panneaux routiers
Cette API expose les endpoints pour l'inf√©rence et le monitoring.
"""

import logging
import time
import io
import uuid
from pathlib import Path
from typing import Dict, List, Optional
import json

from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import numpy as np
from PIL import Image
import cv2

# Import des modules internes
import sys
sys.path.append("src")
from ml_pipelines.inference_pipeline import RoadSignInferencePipeline

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialisation de l'application FastAPI
app = FastAPI(
    title="üö¶ Road Sign ML API",
    description="API pour la d√©tection et reconnaissance de panneaux routiers avec YOLOv8 + OCR",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifier les domaines autoris√©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pipeline global (initialis√© au d√©marrage)
pipeline: Optional[RoadSignInferencePipeline] = None

# Statistiques globales
app_stats = {
    "total_predictions": 0,
    "total_detections": 0,
    "total_processing_time": 0.0,
    "start_time": time.time()
}


# ==========================================
# MOD√àLES PYDANTIC POUR VALIDATION
# ==========================================

class DetectionResult(BaseModel):
    """Mod√®le pour une d√©tection individuelle"""
    bbox: List[int] = Field(..., description="Bounding box [x1, y1, x2, y2]")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confiance de d√©tection")
    class_id: int = Field(..., description="ID de la classe")
    class_name: str = Field(..., description="Nom de la classe")
    ocr: Dict = Field(..., description="R√©sultat OCR")
    has_text: bool = Field(..., description="Pr√©sence de texte d√©tect√©")


class PredictionResponse(BaseModel):
    """Mod√®le pour la r√©ponse de pr√©diction"""
    success: bool = Field(..., description="Succ√®s de la pr√©diction")
    image_shape: List[int] = Field(..., description="Dimensions de l'image [H, W, C]")
    detections_count: int = Field(..., description="Nombre de d√©tections")
    results: List[DetectionResult] = Field(..., description="Liste des d√©tections")
    processing_time: float = Field(..., description="Temps de traitement en secondes")
    pipeline_version: str = Field(..., description="Version du pipeline")
    request_id: str = Field(..., description="ID unique de la requ√™te")


class HealthResponse(BaseModel):
    """Mod√®le pour la r√©ponse de sant√©"""
    status: str = Field(..., description="Statut de l'API")
    pipeline_loaded: bool = Field(..., description="Pipeline charg√©")
    uptime: float = Field(..., description="Temps de fonctionnement en secondes")
    total_predictions: int = Field(..., description="Nombre total de pr√©dictions")
    version: str = Field(..., description="Version de l'API")


class MetricsResponse(BaseModel):
    """Mod√®le pour les m√©triques Prometheus"""
    total_predictions: int
    total_detections: int
    average_processing_time: float
    uptime: float
    pipeline_status: str


# ==========================================
# FONCTIONS UTILITAIRES
# ==========================================

def initialize_pipeline():
    """Initialise le pipeline ML global"""
    global pipeline
    try:
        logger.info("Initialisation du pipeline ML...")
        pipeline = RoadSignInferencePipeline()
        logger.info("‚úÖ Pipeline ML initialis√© avec succ√®s")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erreur initialisation pipeline: {e}")
        return False


def update_stats(detections_count: int, processing_time: float):
    """Met √† jour les statistiques globales"""
    global app_stats
    app_stats["total_predictions"] += 1
    app_stats["total_detections"] += detections_count
    app_stats["total_processing_time"] += processing_time


def process_uploaded_file(file: UploadFile) -> np.ndarray:
    """
    Traite un fichier upload√© et le convertit en image numpy
    
    Args:
        file: Fichier upload√©
        
    Returns:
        np.ndarray: Image sous forme de tableau numpy
    """
    # V√©rification du type de fichier
    if not file.content_type or not file.content_type.startswith('image/'):
        raise HTTPException(
            status_code=400, 
            detail=f"Type de fichier non support√©: {file.content_type}. Utilisez une image."
        )
    
    try:
        # Lecture du fichier
        image_data = file.file.read()
        
        # Conversion en image PIL puis numpy
        pil_image = Image.open(io.BytesIO(image_data))
        
        # Conversion en RGB si n√©cessaire
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        # Conversion en numpy array
        numpy_image = np.array(pil_image)
        
        # Conversion RGB vers BGR pour OpenCV
        bgr_image = cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)
        
        return bgr_image
        
    except Exception as e:
        logger.error(f"Erreur traitement fichier: {e}")
        raise HTTPException(
            status_code=400,
            detail=f"Impossible de traiter l'image: {str(e)}"
        )


# ==========================================
# √âV√âNEMENTS DE CYCLE DE VIE
# ==========================================

@app.on_event("startup")
async def startup_event():
    """Initialisation au d√©marrage de l'application"""
    logger.info("üöÄ D√©marrage de l'API Road Sign ML")
    
    # Cr√©ation des r√©pertoires n√©cessaires
    Path("logs").mkdir(exist_ok=True)
    Path("temp").mkdir(exist_ok=True)
    
    # Initialisation du pipeline
    if not initialize_pipeline():
        logger.warning("‚ö†Ô∏è Pipeline non initialis√© - fonctionnement en mode d√©grad√©")
    
    logger.info("‚úÖ API Road Sign ML d√©marr√©e avec succ√®s")


@app.on_event("shutdown")
async def shutdown_event():
    """Nettoyage √† l'arr√™t de l'application"""
    logger.info("üõë Arr√™t de l'API Road Sign ML")
    
    # Nettoyage des fichiers temporaires
    temp_dir = Path("temp")
    if temp_dir.exists():
        for temp_file in temp_dir.glob("*"):
            try:
                temp_file.unlink()
            except Exception:
                pass
    
    logger.info("‚úÖ API Road Sign ML arr√™t√©e proprement")


# ==========================================
# ROUTES PRINCIPALES
# ==========================================

@app.get("/", response_class=HTMLResponse)
async def root():
    """Page d'accueil avec interface de test"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>üö¶ Road Sign ML API</title>
        <meta charset="utf-8">
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }
            .container { max-width: 800px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
            h1 { color: #2c3e50; text-align: center; }
            .section { margin: 30px 0; padding: 20px; border: 1px solid #ddd; border-radius: 5px; }
            .upload-area { border: 2px dashed #3498db; padding: 40px; text-align: center; border-radius: 10px; }
            .upload-area:hover { background-color: #ecf0f1; }
            input[type="file"] { margin: 20px 0; }
            button { background-color: #3498db; color: white; padding: 12px 24px; border: none; border-radius: 5px; cursor: pointer; font-size: 16px; }
            button:hover { background-color: #2980b9; }
            .result { margin-top: 20px; padding: 15px; background-color: #f8f9fa; border-radius: 5px; }
            .error { background-color: #f8d7da; color: #721c24; }
            .success { background-color: #d4edda; color: #155724; }
            .stats { display: flex; justify-content: space-around; text-align: center; }
            .stat-item { padding: 15px; }
            .stat-number { font-size: 24px; font-weight: bold; color: #3498db; }
            pre { background: #f4f4f4; padding: 15px; border-radius: 5px; overflow-x: auto; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üö¶ Road Sign ML API</h1>
            <p style="text-align: center; color: #7f8c8d;">
                API de d√©tection et reconnaissance de panneaux routiers avec YOLOv8 + OCR
            </p>
            
            <div class="section">
                <h2>üìä Statistiques en temps r√©el</h2>
                <div class="stats" id="stats">
                    <div class="stat-item">
                        <div class="stat-number" id="predictions">-</div>
                        <div>Pr√©dictions</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number" id="detections">-</div>
                        <div>D√©tections</div>
                    </div>
                    <div class="stat-item">
                        <div class="stat-number" id="uptime">-</div>
                        <div>Uptime (min)</div>
                    </div>
                </div>
            </div>
            
            <div class="section">
                <h2>üñºÔ∏è Test de pr√©diction</h2>
                <div class="upload-area">
                    <p>üìÅ Glissez-d√©posez une image ici ou cliquez pour s√©lectionner</p>
                    <input type="file" id="imageInput" accept="image/*" />
                    <br>
                    <button onclick="predictImage()">üîç Analyser l'image</button>
                </div>
                <div id="result" class="result" style="display: none;"></div>
                <div id="imagePreview" style="margin-top: 20px; text-align: center;"></div>
            </div>
            
            <div class="section">
                <h2>üìö Documentation</h2>
                <p>
                    <strong>Endpoints disponibles :</strong><br>
                    ‚Ä¢ <a href="/docs" target="_blank">üìñ Documentation Swagger</a><br>
                    ‚Ä¢ <a href="/health" target="_blank">üíö Health Check</a><br>
                    ‚Ä¢ <a href="/metrics" target="_blank">üìä M√©triques Prometheus</a><br>
                    ‚Ä¢ <code>POST /predict</code> - Pr√©diction sur image<br>
                    ‚Ä¢ <code>POST /predict/batch</code> - Pr√©diction batch
                </p>
            </div>
        </div>
        
        <script>
            // Mise √† jour des statistiques
            async function updateStats() {
                try {
                    const response = await fetch('/health');
                    const data = await response.json();
                    document.getElementById('predictions').textContent = data.total_predictions;
                    document.getElementById('uptime').textContent = Math.round(data.uptime / 60);
                    
                    const metricsResponse = await fetch('/metrics');
                    const metricsData = await metricsResponse.json();
                    document.getElementById('detections').textContent = metricsData.total_detections;
                } catch (error) {
                    console.error('Erreur mise √† jour stats:', error);
                }
            }
            
            // Pr√©diction d'image
            async function predictImage() {
                const fileInput = document.getElementById('imageInput');
                const resultDiv = document.getElementById('result');
                const previewDiv = document.getElementById('imagePreview');
                
                if (!fileInput.files[0]) {
                    alert('Veuillez s√©lectionner une image');
                    return;
                }
                
                const formData = new FormData();
                formData.append('file', fileInput.files[0]);
                
                resultDiv.style.display = 'block';
                resultDiv.className = 'result';
                resultDiv.innerHTML = '‚è≥ Analyse en cours...';
                
                // Aper√ßu de l'image
                const file = fileInput.files[0];
                const reader = new FileReader();
                reader.onload = function(e) {
                    previewDiv.innerHTML = `<img src="${e.target.result}" style="max-width: 400px; max-height: 300px; border-radius: 5px;">`;
                };
                reader.readAsDataURL(file);
                
                try {
                    const response = await fetch('/predict', {
                        method: 'POST',
                        body: formData
                    });
                    
                    const data = await response.json();
                    
                    if (data.success) {
                        resultDiv.className = 'result success';
                        let html = `<h3>‚úÖ Analyse r√©ussie!</h3>
                            <p><strong>D√©tections trouv√©es:</strong> ${data.detections_count}</p>
                            <p><strong>Temps de traitement:</strong> ${data.processing_time.toFixed(3)}s</p>`;
                        
                        if (data.results && data.results.length > 0) {
                            html += '<h4>üìã D√©tails des d√©tections:</h4>';
                            data.results.forEach((result, index) => {
                                html += `
                                    <div style="margin: 10px 0; padding: 10px; background: #e8f4f8; border-radius: 5px;">
                                        <strong>D√©tection ${index + 1}:</strong><br>
                                        ‚Ä¢ Classe: ${result.class_name}<br>
                                        ‚Ä¢ Confiance: ${(result.confidence * 100).toFixed(1)}%<br>
                                        ‚Ä¢ Texte OCR: "${result.ocr.text}"<br>
                                        ‚Ä¢ Confiance OCR: ${(result.ocr.confidence * 100).toFixed(1)}%
                                    </div>
                                `;
                            });
                        }
                        
                        resultDiv.innerHTML = html;
                    } else {
                        resultDiv.className = 'result error';
                        resultDiv.innerHTML = `<h3>‚ùå Erreur</h3><p>${data.error || 'Erreur inconnue'}</p>`;
                    }
                    
                    updateStats();
                    
                } catch (error) {
                    resultDiv.className = 'result error';
                    resultDiv.innerHTML = `<h3>‚ùå Erreur de connexion</h3><p>${error.message}</p>`;
                }
            }
            
            // Mise √† jour initiale et p√©riodique des stats
            updateStats();
            setInterval(updateStats, 10000); // Toutes les 10 secondes
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)


@app.post("/predict", response_model=PredictionResponse)
async def predict_image(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(..., description="Image √† analyser")
):
    """
    Effectue une pr√©diction sur une image upload√©e
    
    Args:
        file: Fichier image (JPEG, PNG, etc.)
        
    Returns:
        PredictionResponse: R√©sultats de la pr√©diction
    """
    if pipeline is None:
        raise HTTPException(
            status_code=503,
            detail="Pipeline ML non initialis√© - service indisponible"
        )
    
    request_id = str(uuid.uuid4())
    start_time = time.time()
    
    try:
        logger.info(f"Nouvelle pr√©diction - ID: {request_id}")
        
        # Traitement du fichier upload√©
        image = process_uploaded_file(file)
        
        # Pr√©diction
        result = pipeline.predict_image(image)
        
        # V√©rification des erreurs
        if 'error' in result:
            raise HTTPException(
                status_code=500,
                detail=f"Erreur de pr√©diction: {result['error']}"
            )
        
        # Mise √† jour des statistiques
        processing_time = result['processing_time']
        detections_count = result['detections_count']
        update_stats(detections_count, processing_time)
        
        # Pr√©paration de la r√©ponse
        response = PredictionResponse(
            success=True,
            image_shape=result['image_shape'],
            detections_count=detections_count,
            results=result['results'],
            processing_time=processing_time,
            pipeline_version=result['pipeline_version'],
            request_id=request_id
        )
        
        # Log asynchrone des m√©triques
        background_tasks.add_task(
            log_prediction_async, 
            request_id, 
            detections_count, 
            processing_time
        )
        
        logger.info(f"Pr√©diction r√©ussie - ID: {request_id} - {detections_count} d√©tections")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur pr√©diction - ID: {request_id}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne: {str(e)}"
        )


@app.post("/predict/batch")
async def predict_batch(
    files: List[UploadFile] = File(..., description="Images √† analyser en batch")
):
    """
    Effectue une pr√©diction sur plusieurs images
    
    Args:
        files: Liste de fichiers images
        
    Returns:
        List[PredictionResponse]: R√©sultats pour chaque image
    """
    if pipeline is None:
        raise HTTPException(
            status_code=503,
            detail="Pipeline ML non initialis√© - service indisponible"
        )
    
    if len(files) > 10:  # Limite pour √©viter la surcharge
        raise HTTPException(
            status_code=400,
            detail="Maximum 10 images par batch"
        )
    
    request_id = str(uuid.uuid4())
    logger.info(f"Pr√©diction batch - ID: {request_id} - {len(files)} images")
    
    results = []
    
    for i, file in enumerate(files):
        try:
            # Traitement de chaque image
            image = process_uploaded_file(file)
            result = pipeline.predict_image(image)
            
            if 'error' not in result:
                update_stats(result['detections_count'], result['processing_time'])
                
                response = PredictionResponse(
                    success=True,
                    image_shape=result['image_shape'],
                    detections_count=result['detections_count'],
                    results=result['results'],
                    processing_time=result['processing_time'],
                    pipeline_version=result['pipeline_version'],
                    request_id=f"{request_id}_{i}"
                )
            else:
                response = PredictionResponse(
                    success=False,
                    image_shape=[0, 0, 0],
                    detections_count=0,
                    results=[],
                    processing_time=0.0,
                    pipeline_version="1.0.0",
                    request_id=f"{request_id}_{i}"
                )
            
            results.append(response)
            
        except Exception as e:
            logger.error(f"Erreur image {i} dans batch {request_id}: {e}")
            results.append({
                "success": False,
                "error": str(e),
                "request_id": f"{request_id}_{i}"
            })
    
    logger.info(f"Batch termin√© - ID: {request_id}")
    return results


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """V√©rification de l'√©tat de sant√© de l'API"""
    uptime = time.time() - app_stats["start_time"]
    
    return HealthResponse(
        status="healthy" if pipeline else "degraded",
        pipeline_loaded=pipeline is not None,
        uptime=uptime,
        total_predictions=app_stats["total_predictions"],
        version="1.0.0"
    )


@app.get("/metrics", response_model=MetricsResponse)
async def get_metrics():
    """M√©triques Prometheus pour monitoring"""
    uptime = time.time() - app_stats["start_time"]
    avg_processing_time = (
        app_stats["total_processing_time"] / app_stats["total_predictions"]
        if app_stats["total_predictions"] > 0 else 0.0
    )
    
    return MetricsResponse(
        total_predictions=app_stats["total_predictions"],
        total_detections=app_stats["total_detections"],
        average_processing_time=avg_processing_time,
        uptime=uptime,
        pipeline_status="loaded" if pipeline else "not_loaded"
    )


# ==========================================
# T√ÇCHES ASYNCHRONES
# ==========================================

async def log_prediction_async(request_id: str, detections_count: int, processing_time: float):
    """Log asynchrone des m√©triques de pr√©diction"""
    try:
        # Log dans les fichiers
        logger.info(f"METRICS - ID: {request_id}, Detections: {detections_count}, Time: {processing_time:.3f}s")
        
        # Ici on pourrait ajouter d'autres syst√®mes de monitoring
        # comme Prometheus, Grafana, etc.
        
    except Exception as e:
        logger.warning(f"Erreur log asynchrone: {e}")


# ==========================================
# POINT D'ENTR√âE
# ==========================================

def run_server(host: str = "0.0.0.0", port: int = 8000, reload: bool = False):
    """
    Lance le serveur FastAPI
    
    Args:
        host: Adresse d'√©coute
        port: Port d'√©coute  
        reload: Rechargement automatique en d√©veloppement
    """
    import uvicorn
    
    logger.info(f"üöÄ Lancement du serveur API sur {host}:{port}")
    uvicorn.run(
        "src.api.main:app" if not reload else "main:app",
        host=host,
        port=port,
        reload=reload,
        log_level="info"
    )


if __name__ == "__main__":
    run_server(reload=True)
